---
layout: post
title: "About GAN (1) interpretation"
date: 2016-10-6
category: Machine Learning
---

# 一点 GAN -之一：简述 及 

### 一些直觉



>  What I cannot create, I do not understand. - Richard Feynman

费曼的这句话可谓脍炙人口，而且，不像大多数名人口中说出的名言一样，它相当有道理。学数学的时候，我常有这样的体会。就算你把整本书背过，如果你并不能完完整整地把每个定理的证明，庖丁解牛成直径一厘米的牛肉粒，then eventually your learn nothing 。

这句话的逆否命题显然有相当多的佐证：你了解四则运算法则，理解语法树，所以可以写出程序来 parse 一个算式，进而「创造」一个计算器；你知道🐰兔子的样子，所以你可以纸上勾勒出一只兔子，不管是躺着的还是坐着的，荷兰兔还是垂耳兔，只要你想画一只兔子，那么画出来的都是兔子。

![alt text][logo]

> credit to 

那么，你的知识里关于兔子的那一部分是什么呢？



没准可以看做是一个函数。



你可以想象成，你是一个「兔子」生成器，而你生成的兔子显然不是一成不变的，因为几乎所有事都可能影响你在下笔时从尾巴或耳朵开始着笔---如果借用计算机的术语，你接受的一切影响在你的内部形成了一个「熵池」，这个「熵池」为你的创作源源不断地提供随机因素—譬如下笔的位置，你想画的兔子种类，是否在吃胡萝卜…这个「熵池」对你的影响，显然是一个概率分布，经过你（一个兔子生成函数）的变换，变成了你生成的兔子的概率分布。

这还没完，大家都会画兔子，但是你们会画皮卡丘吗？

![alt text][pikachu]



一般人会做的事情：照着教程或者凭空想象->试着画几笔->发现不像->google pikachu->对着皮卡丘改动自己的画->就这样吧

总结下来，就是根据真实的样本，不断修改自己的画作，同时修正脑海中的记忆（参数），直到自己画的皮卡丘看起来跟真的差不多为止。

That‘s basically what GAN ( Generative Adverserial Network ) does.

注意，以上的类比绝不准确，任何类比都不准确。但这可能为那些想知道 GAN 在做什么，而又不想看数学的人一个直觉，即：GAN 在用一个 *parameterized distribution* 逼近真实样本的分布。

### 一些架构

GAN 通常分为两个部分，generator ( denoted as $G$ ) ， discriminator (denoted as $D$ ) 。

其中 $G$ 的作用就是通过接收一个 ramdom noise ( often denoted as $z$ )， 来生成一个样本的概率分布 $G(z;\psi)$，其中 $\psi$ 是 $G$ 的参数。$G$ 通常是一个 deconvolution net， 例如 DCGAN 里的 generator 。

而 $D$ 的作用则是一个二分类器，可能的实现有：MLP (vanilla version) ；一个 CNN extractor ，后接一个 sigmoid ；或者一个 autoencoder+重建损失 (in [EBGAN][2], we will reach it later)  。$D$ 的作用是区别 $G$ 生成的样本与真实样本。我们用 $D(x)$ 代表这个二分类器输出的 $x$ 为正例（真实样本）的概率。 

理论上来说，GAN 的训练应该分两步：

* 为了用 $G$ 近似真实样本的概率分布，我们首先应该用真实样本和生成样本训练D，以期得到一个可以完美分类真实样本和生成样本的分类器。假设我们训练得到了这个分类器，那么此时D(x)就是x是正例的概率。通过训练，我们实际上得到了$P(c|X)$， 其中c为类别（正例或负例）， X为可能的样本。
* 为了得到$P(X|c=1)$ , 继续训练 $G$ 来最小化$E_{z∼p_z(z)}(1-P(c=1|G(z)))$ 。如果训练完美， 那么此时$G(z), z∼some\ distribution$ 就是$P(X|c=1)$

在最初 *Ian et all* 的 [Generative Adversarial Nets][1] 里，把 GAN 形容为一个 minimax game （极大极小博弈），按照这种说法，GAN 要做的就是下面的这个任务：

​     $\underset{G}{min}\underset{D}max V (D, G) = E_{x∼p_{data}(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 − D(G(z)))]$

这跟以上两步训练过程一模一样，内层的最大化就是第一步，做的其实是最小化 $D(X)$ 与 $P(c|X)$ 的 cross entropy loss，外层的最小化就是第二步， 做的是在最小化 $E_{z∼p_z(z)}(1-P(c=1|G(z)))$ 。

你尽可以将以上的训练过程与*一些直觉* 里的例子进行类比



[logo]:http://xm.xuelema.com/attached/image/20140523/20140523152438_5841.jpg
[pikachu]:picachu.jpg
[1]:https://arxiv.org/pdf/1406.2661v1.pdf
[2]:https://arxiv.org/abs/1609.03126



